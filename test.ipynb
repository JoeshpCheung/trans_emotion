{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43a7e4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"1\"\n",
    "import tensorflow as tf\n",
    "\n",
    "from transformers import AutoTokenizer, TFAutoModelForPreTraining\n",
    "# from transformers import TFElectraForSequenceClassification, ElectraConfig\n",
    "from transformers import TFAutoModelForSequenceClassification, AutoConfig\n",
    "from transformers import TFTrainer, TFTrainingArguments\n",
    "from transformers import training_args\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import yaml\n",
    "\n",
    "config = yaml.load(open('config.yaml', 'r'), Loader=yaml.FullLoader)\n",
    "\n",
    "# model = TFAutoModelForPreTraining.from_pretrained(\"hfl/chinese-electra-180g-small-discriminator\")\n",
    "# model = TFElectraForSequenceClassification.from_pretrained(\"hfl/chinese-electra-180g-small-discriminator\")\n",
    "\n",
    "# inputs = tokenizer(\"‰Ω†Âê¨ÊòéÁôΩ‰∫ÜÂêó\", return_tensors=\"tf\")\n",
    "# outputs = model(**inputs)\n",
    "# print(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fd06d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config['pretrain_model_dir'])\n",
    "# tokenizer = AutoTokenizer.from_pretrained('/home/jasoncheung/project/trans/trans_models/trans_datas/pretrained_models/roberta_chinese_4_512')\n",
    "# load datas\n",
    "path_datas = '/home/jasoncheung/project/trans/trans_datas/weibo_senti_100k.csv'\n",
    "df = pd.read_csv(path_datas)\n",
    "datas = df.review.tolist()\n",
    "labels = df.label.tolist()\n",
    "\n",
    "train_datas, test_datas, train_labels, test_labels = train_test_split(datas, labels, test_size=0.1)\n",
    "train_datas, val_datas, train_labels, val_labels = train_test_split(train_datas, train_labels, test_size=0.1)\n",
    "\n",
    "train_encodings = tokenizer(train_datas, return_tensors=\"tf\", truncation=True, padding='max_length', max_length=150)\n",
    "val_encodings = tokenizer(val_datas, return_tensors=\"tf\", truncation=True, padding='max_length', max_length=150)\n",
    "test_encodings = tokenizer(test_datas, return_tensors=\"tf\", truncation=True, padding='max_length', max_length=150)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), train_labels))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((dict(val_encodings), val_labels))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b33fe117",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ./trans_datas/pretrained_models/electra_chinese_small were not used when initializing TFElectraForSequenceClassification: ['discriminator_predictions']\n",
      "- This IS expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFElectraForSequenceClassification were not initialized from the model checkpoint at ./trans_datas/pretrained_models/electra_chinese_small and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "training_args = TFTrainingArguments(\n",
    "    do_train=config['do_train'],\n",
    "    do_eval=config['do_eval'],\n",
    "    output_dir=config['output_dir'],          # output directory\n",
    "    num_train_epochs=config['num_train_epochs'],              # total # of training epochs\n",
    "    per_device_train_batch_size=config['train_batch_size'],  # batch size per device during training\n",
    "    per_device_eval_batch_size=config['eval_batch_size'],   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir=config['logging_dir'],            # directory for storing logs\n",
    "    save_total_limit=config['save_total_limit'],\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=config['eval_steps'],\n",
    "    load_best_model_at_end=True,\n",
    "    disable_tqdm=False,\n",
    "    max_steps=config['max_steps'],\n",
    "    save_steps=config['save_steps'],\n",
    "\n",
    ")\n",
    "\n",
    "# load model\n",
    "\n",
    "with training_args.strategy.scope(): \n",
    "    model = TFAutoModelForSequenceClassification.from_pretrained(config['pretrain_model_dir'], \n",
    "                                                                 num_labels=config['num_labels'], )\n",
    "    # ÈîÅ‰ΩèelectraÂ±ÇÔºåÂè™ËÆ≠ÁªÉËæìÂá∫Â±Ç, ‰ºöÊØîÂÖ®ËÆ≠ÁªÉÂø´3ÂÄçÂ∑¶Âè≥\n",
    "    if config['model_name'] == 'electra':\n",
    "        model.electra.trainable=False\n",
    "    elif config['model_name'] == 'bert':\n",
    "        model.roberta.trainable=False\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "trainer = TFTrainer(\n",
    "    model=model,                         # the instantiated ü§ó Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,    # ensorflow_datasets training dataset\n",
    "    eval_dataset=val_dataset,       # tensorflow_datasets evaluation dataset\n",
    "    compute_metrics=compute_metrics,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d5e6cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f4ce6d96fa0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f4ce6d96fa0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:From /home/jasoncheung/.virtualenvs/tf_transformers/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:5043: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jasoncheung/.virtualenvs/tf_transformers/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/jasoncheung/.virtualenvs/tf_transformers/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time use:  11668.286463022232\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "tic = time.time()\n",
    "trainer.train()\n",
    "toc = time.time()\n",
    "print('time use: ', toc - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad2dcc45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embeddings_layer_call_and_return_conditional_losses, embeddings_layer_call_fn, embeddings_project_layer_call_and_return_conditional_losses, embeddings_project_layer_call_fn, encoder_layer_call_and_return_conditional_losses while saving (showing 5 of 1065). These functions will not be directly callable after loading.\n",
      "/home/jasoncheung/.virtualenvs/tf_transformers/lib/python3.8/site-packages/tensorflow/python/keras/utils/generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./results/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./results/model/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('./results/model')\n",
    "# model = TFElectraForSequenceClassification.from_pretrained(dir_path, \n",
    "#                                                            num_labels=2, \n",
    "#                                                            )\n",
    "# model.load_weights('./results/model/')\n",
    "# model.load_weights('./results/checkpoint/ckpt-6.index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0dc4f155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    }
   ],
   "source": [
    "res = trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "975ee0f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6538525033504405,\n",
       " 'eval_accuracy': 0.6857546542553191,\n",
       " 'eval_f1': 0.679549114331723,\n",
       " 'eval_precision': 0.7034567467976838,\n",
       " 'eval_recall': 0.6572131147540984}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72a133d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Allocate a pipeline for sentiment-analysis\n",
    "classifier = pipeline('sentiment-analysis')\n",
    "classifier('We are very happy to introduce pipeline to the transformers repository.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdae9e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = '/home/jasoncheung/project/trans/trans_models/sentiment_analysis/'\n",
    "classifier.save_pretrained(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517ec175",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "transformers.TextClassificationPipeline??"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
